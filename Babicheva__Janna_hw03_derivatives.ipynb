{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2iVbLYUAa8ONXrZ/BNtST",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JannaBabicheva/Machine-learning/blob/main/Babicheva__Janna_hw03_derivatives.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы завершить класс Loss и Derivatives, вам необходимо реализовать статические методы для mae, l2_reg, l1_reg и их соответствующих производных."
      ],
      "metadata": {
        "id": "KQGzsg00nWMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "HGg9djxzo8G8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossAndDerivatives:\n",
        "    @staticmethod\n",
        "    def mse(X, Y, w):\n",
        "        return np.mean((X.dot(w) - Y)**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def mae(X, Y, w):\n",
        "        \"\"\" Mean Absolute Error \"\"\"\n",
        "        return np.mean(np.abs(X.dot(w) - Y))\n",
        "\n",
        "    @staticmethod\n",
        "    def l2_reg(w):\n",
        "        \"\"\" L2 Regularization \"\"\"\n",
        "        return np.sum(w**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def l1_reg(w):\n",
        "        \"\"\" L1 Regularization \"\"\"\n",
        "        return np.sum(np.abs(w))\n",
        "\n",
        "    @staticmethod\n",
        "    def no_reg(w):\n",
        "        \"\"\"\n",
        "        Simply ignores the regularization\n",
        "        \"\"\"\n",
        "        return 0.\n",
        "\n",
        "    @staticmethod\n",
        "    def mse_derivative(X, Y, w):\n",
        "        \"\"\" Derivative of MSE with respect to weights w \"\"\"\n",
        "        return (2 / X.shape[0]) * X.T.dot(X.dot(w) - Y)\n",
        "\n",
        "    @staticmethod\n",
        "    def mae_derivative(X, Y, w):\n",
        "        \"\"\" Derivative of MAE (not differentiable at zero, sub-gradient) \"\"\"\n",
        "        return np.sign(X.dot(w) - Y).dot(X) / X.shape[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def l2_reg_derivative(w):\n",
        "        \"\"\" Derivative of L2 regularization \"\"\"\n",
        "        return 2 * w\n",
        "\n",
        "    @staticmethod\n",
        "    def l1_reg_derivative(w):\n",
        "        \"\"\" Derivative of L1 regularization (sub-gradient) \"\"\"\n",
        "        return np.sign(w)\n",
        "\n",
        "    @staticmethod\n",
        "    def no_reg_derivative(w):\n",
        "        \"\"\"\n",
        "        Simply ignores the derivative\n",
        "        \"\"\"\n",
        "        return np.zeros_like(w)"
      ],
      "metadata": {
        "id": "w5PExj6XmoC4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "MAE (Средняя абсолютная ошибка): Вычисляет среднее значение абсолютных различий между прогнозируемыми значениями и истинными значениями.\n",
        "\n",
        "L2 Regularization: Также известная как регуляризация гребня, она добавляет квадраты значений коэффициентов в качестве штрафа к потере.\n",
        "\n",
        "L1 Regularization: Также известная как регуляризация Лассо, она добавляет абсолютную величину значений коэффициента в качестве штрафа.\n",
        "\n",
        "Производные: Каждый из методов для производных вычисляет градиент по отношению к весам, это имеет решающее значение для процедур оптимизации, таких как градиентный спуск. Так же производная MAE и регуляризация L1 являются субградиентами, равными нулю."
      ],
      "metadata": {
        "id": "vOR45mKlr2II"
      }
    }
  ]
}